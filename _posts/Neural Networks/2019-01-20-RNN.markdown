---
title: "Recurrent Neural Networks"
date: 2019-01-20
header:
  overlay_image: /images/Neural Networks/RNN/model.jpeg
  overlay_filter: 0.5
  actions:
    - label: "Full Code"
      url: "https://github.com/najeesmith"
---
# Introduction
My personal favorite neural network. It solved the vanishing gradient problem
by avoiding it all together and is incredibly useful for time-dependent data.

# Code
```python
#Building RNN
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

regressor = Sequential()
#Layer 1
regressor.add(LSTM(units = 65, return_sequences = True, input_shape =(X_train.shape[1], 1)))
#Dropout regularization
regressor.add(Dropout(0.2))

#Layer 2
regressor.add(LSTM(units = 65, return_sequences = True))
regressor.add(Dropout(0.2))

#Layer 3
regressor.add(LSTM(units = 65, return_sequences = True))
regressor.add(Dropout(0.2))

#Layer 4
regressor.add(LSTM(units = 65, return_sequences = False))
regressor.add(Dropout(0.2))

#Adding output layer
regressor.add(Dense(units = 1))

#Compile
regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')
```


# Results
