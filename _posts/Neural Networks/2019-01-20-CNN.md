---
title: "Convolutional Neural Networks"
date: 2019-01-20
excerpt: ""
header:
  overlay_image: /images/Neural Networks/CNN/model.png
  overlay_filter: 0.5
  actions:
    - label: "Full Code"
      url: "https://github.com/najeesmith"
---
# Introduction
When I first began my machine learning journey, I had read one intro article, but got too excited and wanted to hurry and create an algorithm already. My first idea was a dog breed recognizer. It wasn't the most original idea, but I was more excited to get my feet wet and understand the process. It was a great idea until I found out that I need thousands of images of every breed at the time, and I only had about -5 MB of space at the time. I just changed it to a dog or cat classifier for the ease of space and so the model has 2 very different objects to ensure that it yields a high accuracy.

# Code
The process was broken into two steps: creating the CNN, then fitting it to the data.

```python
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Convolution2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten

#Initializing the CNN
classifier = Sequential()

#Convolution Cweation
classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3 ), activation = 'relu'))

classifier.add(MaxPooling2D(pool_size = (2, 2)))

classifier.add(Flatten())

classifier.add(Dense(output_dim = 128, activation = 'relu'))
classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))

classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
```
In short, my understanding of how CNN's work can be broken down into the following:

* Sequential: Initializes the network

* Convolution: Turning the image into a 64x64 arrays to try to find features that make whatever that's in the image unique

* Pooling: Reducing the side of the computed array into a 2x2 matrix that takes the maximum value from the previous layer

* Flattening: Concatenates the pooling layers into a 1-D matrix

* Dense: The penultimate and ultimate layers where the predictions are made based off the prior nodes 

```python

#Fitting
from keras.preprocessing.image import ImageDataGenerator
train_datagen = ImageDataGenerator(
        rescale=1./255, #These lines change the image sizes into similar shapes
        shear_range=0.2, #This will make very beautiful images that will be accepted by the model
        zoom_range=0.2,
        horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1./255)

training_set = train_datagen.flow_from_directory(
        'training_set',
        target_size=(64, 64), #Changes size of each image to match convolution layer
        batch_size=32, #How many will be tested at time
        class_mode='binary') #Binary since either cat or dog

test_set = test_datagen.flow_from_directory(
        'test_set',
        target_size=(64, 64),
        batch_size=32,
        class_mode='binary')

classifier.fit_generator( #Training the model with the training images
        training_set,
        steps_per_epoch=5000, #Arbitrary number. Didn't want too high because my laptop was not too powerful
        epochs=20,
        validation_data=test_set, #Allows predictions to be checked for correctness
        validation_steps=800)
```

# Results
I was elated with my results especially since it was my first real run with machine or deep learning. It earnestly gave me a great sense of accomplishment completing it rather than the results. Speaking of which, came out well.

<img src="/images\Neural Networks\CNN\results.PNG">
